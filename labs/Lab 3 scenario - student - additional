{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopia Lab 3 scenario - student","provenance":[{"file_id":"1mbRybTuEd5hfMgPq47jAy40aizNX03x8","timestamp":1636489869636},{"file_id":"1hs2ViNkY7vFE7l_PL7b-2XIN17cxbsyL","timestamp":1635823858600},{"file_id":"1t76la2tUWVLnEK7IKxdFZn_Y0be3xZud","timestamp":1635823610862}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f52b4e3f89254ae1b5234fbefca2e552":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ee7e1eeaac8342f69582c144cb8f1df3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d1174b8773a34a49b3fd488faff15436","IPY_MODEL_91c17d708c794487ab2f431b540d52fb","IPY_MODEL_c52d9532308f409b8952395a40d815b8"]}},"ee7e1eeaac8342f69582c144cb8f1df3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1174b8773a34a49b3fd488faff15436":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9d1531f60dca4abeb08ca21968e0d1ed","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_82cc174db1cb4f928aa5f813d7f3449f"}},"91c17d708c794487ab2f431b540d52fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c1f28d054d7d40c68704e20bf1a210e7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c44e2e0bcce542f4a7cc6fd224e7d0dc"}},"c52d9532308f409b8952395a40d815b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_37287986150a4e25b92b1c8d87985239","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 43671559.23it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0772f0b7147149738f64f8d848e49d61"}},"9d1531f60dca4abeb08ca21968e0d1ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"82cc174db1cb4f928aa5f813d7f3449f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1f28d054d7d40c68704e20bf1a210e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c44e2e0bcce542f4a7cc6fd224e7d0dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"37287986150a4e25b92b1c8d87985239":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0772f0b7147149738f64f8d848e49d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c94f3b6ac7d149079cf6b11cf1d12b1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_56f3d075ff2642b88119aba263742cf9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d7a326951c6d47289694101bcf691107","IPY_MODEL_74383d9147594338ba08358677f4467d","IPY_MODEL_0e11de3e423a4b01acd653a7832924bd"]}},"56f3d075ff2642b88119aba263742cf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7a326951c6d47289694101bcf691107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3c10fb06422247d78d217cc34c12a2e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5093c94b73664c9897ac920796e416df"}},"74383d9147594338ba08358677f4467d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d15e7ca50ff3445a93758ec963fd97e4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_667273612dba44818971a93f6c3b50cb"}},"0e11de3e423a4b01acd653a7832924bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_147a930944c94543be7bf601b8dfad9e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 704411.56it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac75e99b9df246489684b022e36b24e7"}},"3c10fb06422247d78d217cc34c12a2e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5093c94b73664c9897ac920796e416df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d15e7ca50ff3445a93758ec963fd97e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"667273612dba44818971a93f6c3b50cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"147a930944c94543be7bf601b8dfad9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac75e99b9df246489684b022e36b24e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec3ed81aa2f340f78ee0c2ee2d2648ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_381b69cff9cd4b2ba02f0320145276d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_302e1ec8686a4a6289a4404d59d72b7c","IPY_MODEL_94340cacc6e640f8bf8f94663d021471","IPY_MODEL_8251b81db4cb41d1a0e1d2630d7e0b51"]}},"381b69cff9cd4b2ba02f0320145276d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"302e1ec8686a4a6289a4404d59d72b7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_291042c909c944deb8932c83d21bc468","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39b3c885f3dc46d0985997d259031614"}},"94340cacc6e640f8bf8f94663d021471":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e7c9598e41664fd38f5840514573b91b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a623114e7600419d8ceb5eebd5599cc3"}},"8251b81db4cb41d1a0e1d2630d7e0b51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_32bbe95bd2e34284a588f0a2cef9e243","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:00&lt;00:00, 18101269.89it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_198e6714b94348c880f32c61c9ec8180"}},"291042c909c944deb8932c83d21bc468":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"39b3c885f3dc46d0985997d259031614":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e7c9598e41664fd38f5840514573b91b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a623114e7600419d8ceb5eebd5599cc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"32bbe95bd2e34284a588f0a2cef9e243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"198e6714b94348c880f32c61c9ec8180":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b15c86d2a65e4d048cc99ac3add8e13e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ac4ae57cc34b43f7b9d1f9a28990df59","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c14fc7f06b2048b4ba9aaa9c8b132198","IPY_MODEL_dae1deb9be9a42bd83e836cc522bb8aa","IPY_MODEL_6cd7c800db24468996f60cd043fc0913"]}},"ac4ae57cc34b43f7b9d1f9a28990df59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c14fc7f06b2048b4ba9aaa9c8b132198":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b6a6a69f84e74a6e90c680859ac7d962","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f88039aab1d9496c9451f468896d91c4"}},"dae1deb9be9a42bd83e836cc522bb8aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cba2ce00cf1f40f988d10f9fcf5c1e35","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79c979f4a496411aaa4c2efd932c0632"}},"6cd7c800db24468996f60cd043fc0913":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6a86632a051d4d5788f0f2f1cf98d2fd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:00&lt;00:00, 130278.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e27712d12324decaeff9432b0622da0"}},"b6a6a69f84e74a6e90c680859ac7d962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f88039aab1d9496c9451f468896d91c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cba2ce00cf1f40f988d10f9fcf5c1e35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79c979f4a496411aaa4c2efd932c0632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a86632a051d4d5788f0f2f1cf98d2fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e27712d12324decaeff9432b0622da0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"z8dpwZEbxSSW"},"source":["https://github.com/ilguyi/optimizers.numpy\n","\n","https://ruder.io/optimizing-gradient-descent/index.html#nesterovacceleratedgradient"]},{"cell_type":"markdown","metadata":{"id":"ziZ9i7tXbO1T"},"source":["In this lab, you will implement some of the techniques discussed in the lecture.\n","\n","Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n"," * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n"," * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence \n","\n","**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two function as a single block and not even compute the gradient over the softmax values. \n","\n","**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n","\n","**Task 3 (optional).** Implement Adagrad, dropout and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.\n","\n","**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n"]},{"cell_type":"code","metadata":{"id":"P22HqX9AbO1a"},"source":["import random\n","import numpy as np\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9jGPaZhbO2B","colab":{"base_uri":"https://localhost:8080/","height":414,"referenced_widgets":["f52b4e3f89254ae1b5234fbefca2e552","ee7e1eeaac8342f69582c144cb8f1df3","d1174b8773a34a49b3fd488faff15436","91c17d708c794487ab2f431b540d52fb","c52d9532308f409b8952395a40d815b8","9d1531f60dca4abeb08ca21968e0d1ed","82cc174db1cb4f928aa5f813d7f3449f","c1f28d054d7d40c68704e20bf1a210e7","c44e2e0bcce542f4a7cc6fd224e7d0dc","37287986150a4e25b92b1c8d87985239","0772f0b7147149738f64f8d848e49d61","c94f3b6ac7d149079cf6b11cf1d12b1b","56f3d075ff2642b88119aba263742cf9","d7a326951c6d47289694101bcf691107","74383d9147594338ba08358677f4467d","0e11de3e423a4b01acd653a7832924bd","3c10fb06422247d78d217cc34c12a2e4","5093c94b73664c9897ac920796e416df","d15e7ca50ff3445a93758ec963fd97e4","667273612dba44818971a93f6c3b50cb","147a930944c94543be7bf601b8dfad9e","ac75e99b9df246489684b022e36b24e7","ec3ed81aa2f340f78ee0c2ee2d2648ac","381b69cff9cd4b2ba02f0320145276d8","302e1ec8686a4a6289a4404d59d72b7c","94340cacc6e640f8bf8f94663d021471","8251b81db4cb41d1a0e1d2630d7e0b51","291042c909c944deb8932c83d21bc468","39b3c885f3dc46d0985997d259031614","e7c9598e41664fd38f5840514573b91b","a623114e7600419d8ceb5eebd5599cc3","32bbe95bd2e34284a588f0a2cef9e243","198e6714b94348c880f32c61c9ec8180","b15c86d2a65e4d048cc99ac3add8e13e","ac4ae57cc34b43f7b9d1f9a28990df59","c14fc7f06b2048b4ba9aaa9c8b132198","dae1deb9be9a42bd83e836cc522bb8aa","6cd7c800db24468996f60cd043fc0913","b6a6a69f84e74a6e90c680859ac7d962","f88039aab1d9496c9451f468896d91c4","cba2ce00cf1f40f988d10f9fcf5c1e35","79c979f4a496411aaa4c2efd932c0632","6a86632a051d4d5788f0f2f1cf98d2fd","0e27712d12324decaeff9432b0622da0"]},"executionInfo":{"status":"ok","timestamp":1636925072704,"user_tz":-60,"elapsed":1653,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"3b6627f2-736c-4a24-d036-9234c7ca6533"},"source":["# Let's read the mnist dataset\n","\n","def load_mnist(path='.'):\n","    train_set = datasets.MNIST(path, train=True, download=True)\n","    x_train = train_set.data.numpy()\n","    _y_train = train_set.targets.numpy()\n","    \n","    test_set = datasets.MNIST(path, train=False, download=True)\n","    x_test = test_set.data.numpy()\n","    _y_test = test_set.targets.numpy()\n","    \n","    x_train = x_train.reshape((x_train.shape[0],28*28)) / 255.\n","    x_test = x_test.reshape((x_test.shape[0],28*28)) / 255.\n","\n","    y_train = np.zeros((_y_train.shape[0], 10))\n","    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n","    \n","    y_test = np.zeros((_y_test.shape[0], 10))\n","    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n","\n","    return (x_train, y_train), (x_test, y_test)\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f52b4e3f89254ae1b5234fbefca2e552","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94f3b6ac7d149079cf6b11cf1d12b1b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec3ed81aa2f340f78ee0c2ee2d2648ac","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b15c86d2a65e4d048cc99ac3add8e13e","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]}]},{"cell_type":"code","metadata":{"id":"f7Oya8NhIzQr"},"source":["# standarization\n","mean = x_train.mean(axis=0)\n","std = x_train.std(axis=0)\n","\n","wrong_features = (std != 0)[0]\n","\n","x_train = (x_train - mean) / std #/np.sqrt(x_train.shape[0]) ?\n","x_test = (x_test - mean) / std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3gAyqw4bO1p"},"source":["clip = 5000\n","def sigmoid(z):\n","  # z = np.clip(z, -clip, clip)\n","  return 1.0 / (1.0 + np.exp(-z))\n","\n","def sigmoid_prime(z):\n","    # Derivative of the sigmoid\n","    return sigmoid(z)*(1-sigmoid(z))\n","\n","def softmax(x):\n","  max_x = np.max(x, axis=0)[np.newaxis,:]\n","  exp_x = np.exp(x-max_x)\n","  return exp_x / exp_x.sum(axis=0)[np.newaxis,:]\n","\n","def relu(x):\n","  return np.maximum(x,0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z2mlUUsoU2C5"},"source":["L1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRbaDahxU2RC","executionInfo":{"status":"ok","timestamp":1636816473251,"user_tz":-60,"elapsed":111217,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"c0139e17-fe5e-4ade-aa8d-31f32590f80e"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.alpha = 0.0001\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        # wagi mniejsze co do wartosci bezwzględnej niż alpha są zerowane\n","        self.weights = [w-eta*self.alpha*np.sign(np.where(abs(w)>self.alpha,w,0))-(eta/len(x_batch))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            idx = np.random.permutation(x_train.shape[0])\n","            x_train = x_train[idx]\n","            y_train = y_train[idx]\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","            # for batch_idx in stratified_split(y_train, mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","                # self.update_mini_batch(x_train[batch_idx], y_train[batch_idx], eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.5, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8196\n","Epoch: 1, Accuracy: 0.8601\n","Epoch: 2, Accuracy: 0.8809\n","Epoch: 3, Accuracy: 0.8923\n","Epoch: 4, Accuracy: 0.899\n","Epoch: 5, Accuracy: 0.9018\n","Epoch: 6, Accuracy: 0.9056\n","Epoch: 7, Accuracy: 0.9097\n","Epoch: 8, Accuracy: 0.9119\n","Epoch: 9, Accuracy: 0.9146\n","Epoch: 10, Accuracy: 0.9168\n","Epoch: 11, Accuracy: 0.9193\n","Epoch: 12, Accuracy: 0.9207\n","Epoch: 13, Accuracy: 0.9233\n","Epoch: 14, Accuracy: 0.9232\n","Epoch: 15, Accuracy: 0.9249\n","Epoch: 16, Accuracy: 0.9248\n","Epoch: 17, Accuracy: 0.9281\n","Epoch: 18, Accuracy: 0.9277\n","Epoch: 19, Accuracy: 0.9281\n","Epoch: 20, Accuracy: 0.9317\n","Epoch: 21, Accuracy: 0.9312\n","Epoch: 22, Accuracy: 0.9314\n","Epoch: 23, Accuracy: 0.9332\n","Epoch: 24, Accuracy: 0.9321\n","Epoch: 25, Accuracy: 0.9348\n","Epoch: 26, Accuracy: 0.935\n","Epoch: 27, Accuracy: 0.934\n","Epoch: 28, Accuracy: 0.9348\n","Epoch: 29, Accuracy: 0.9365\n","Epoch: 30, Accuracy: 0.9366\n","Epoch: 31, Accuracy: 0.9361\n","Epoch: 32, Accuracy: 0.9368\n","Epoch: 33, Accuracy: 0.9395\n","Epoch: 34, Accuracy: 0.9377\n","Epoch: 35, Accuracy: 0.9395\n","Epoch: 36, Accuracy: 0.938\n","Epoch: 37, Accuracy: 0.9396\n","Epoch: 38, Accuracy: 0.9395\n","Epoch: 39, Accuracy: 0.9394\n","Epoch: 40, Accuracy: 0.9407\n","Epoch: 41, Accuracy: 0.9396\n","Epoch: 42, Accuracy: 0.9406\n","Epoch: 43, Accuracy: 0.9404\n","Epoch: 44, Accuracy: 0.9412\n","Epoch: 45, Accuracy: 0.9392\n","Epoch: 46, Accuracy: 0.9416\n","Epoch: 47, Accuracy: 0.9412\n","Epoch: 48, Accuracy: 0.9407\n","Epoch: 49, Accuracy: 0.9415\n","Epoch: 50, Accuracy: 0.9405\n","Epoch: 51, Accuracy: 0.9425\n","Epoch: 52, Accuracy: 0.9435\n","Epoch: 53, Accuracy: 0.9426\n","Epoch: 54, Accuracy: 0.9422\n","Epoch: 55, Accuracy: 0.9444\n","Epoch: 56, Accuracy: 0.9427\n","Epoch: 57, Accuracy: 0.9443\n","Epoch: 58, Accuracy: 0.9448\n","Epoch: 59, Accuracy: 0.9458\n","Epoch: 60, Accuracy: 0.9453\n","Epoch: 61, Accuracy: 0.9462\n","Epoch: 62, Accuracy: 0.9458\n","Epoch: 63, Accuracy: 0.9459\n","Epoch: 64, Accuracy: 0.9463\n","Epoch: 65, Accuracy: 0.9446\n","Epoch: 66, Accuracy: 0.9465\n","Epoch: 67, Accuracy: 0.9463\n","Epoch: 68, Accuracy: 0.9459\n","Epoch: 69, Accuracy: 0.9457\n","Epoch: 70, Accuracy: 0.9462\n","Epoch: 71, Accuracy: 0.9449\n","Epoch: 72, Accuracy: 0.9464\n","Epoch: 73, Accuracy: 0.9476\n","Epoch: 74, Accuracy: 0.9477\n","Epoch: 75, Accuracy: 0.947\n","Epoch: 76, Accuracy: 0.9454\n","Epoch: 77, Accuracy: 0.9476\n","Epoch: 78, Accuracy: 0.9491\n","Epoch: 79, Accuracy: 0.9467\n","Epoch: 80, Accuracy: 0.9468\n","Epoch: 81, Accuracy: 0.9464\n","Epoch: 82, Accuracy: 0.9471\n","Epoch: 83, Accuracy: 0.9487\n","Epoch: 84, Accuracy: 0.948\n","Epoch: 85, Accuracy: 0.9482\n","Epoch: 86, Accuracy: 0.9486\n","Epoch: 87, Accuracy: 0.9473\n","Epoch: 88, Accuracy: 0.9492\n","Epoch: 89, Accuracy: 0.9477\n","Epoch: 90, Accuracy: 0.9486\n","Epoch: 91, Accuracy: 0.9508\n","Epoch: 92, Accuracy: 0.9498\n","Epoch: 93, Accuracy: 0.9495\n","Epoch: 94, Accuracy: 0.9498\n","Epoch: 95, Accuracy: 0.9497\n","Epoch: 96, Accuracy: 0.9495\n","Epoch: 97, Accuracy: 0.9499\n","Epoch: 98, Accuracy: 0.9495\n","Epoch: 99, Accuracy: 0.9496\n"]}]},{"cell_type":"markdown","metadata":{"id":"GF4UqedaNtab"},"source":["Adagrad"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ULDAo2cNtBU","executionInfo":{"status":"ok","timestamp":1636736349761,"user_tz":-60,"elapsed":68614,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"c2757460-d83b-4b87-eeb3-d499ac182500"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.m_b = [np.zeros_like(w) for w in self.biases]\n","        self.m_w = [np.zeros_like(w) for w in self.weights]\n","        self.eps = 1e-8\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","\n","        self.m_b = [m + (nb/len(x_batch))**2 for m, nb in zip(self.m_b, nabla_b)]\n","        self.biases = [b - (eta / np.sqrt(m + self.eps)) * nb / len(x_batch)\\\n","                       for b, nb, m in zip(self.biases, nabla_b, self.m_b)]\n","\n","        self.m_w = [m + (nb/len(x_batch))**2 for m, nb in zip(self.m_w, nabla_w)]\n","        self.weights = [w - (eta/np.sqrt(m + self.eps)) * nw / len(x_batch)\\\n","                        for w, nw, m in zip(self.weights, nabla_w, self.m_w)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.1, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.9008\n","Epoch: 1, Accuracy: 0.914\n","Epoch: 2, Accuracy: 0.9198\n","Epoch: 3, Accuracy: 0.9248\n","Epoch: 4, Accuracy: 0.9295\n","Epoch: 5, Accuracy: 0.9314\n","Epoch: 6, Accuracy: 0.9334\n","Epoch: 7, Accuracy: 0.9358\n","Epoch: 8, Accuracy: 0.9367\n","Epoch: 9, Accuracy: 0.9376\n","Epoch: 10, Accuracy: 0.9393\n","Epoch: 11, Accuracy: 0.94\n","Epoch: 12, Accuracy: 0.9411\n","Epoch: 13, Accuracy: 0.9417\n","Epoch: 14, Accuracy: 0.9425\n","Epoch: 15, Accuracy: 0.9435\n","Epoch: 16, Accuracy: 0.9442\n","Epoch: 17, Accuracy: 0.9452\n","Epoch: 18, Accuracy: 0.9458\n","Epoch: 19, Accuracy: 0.9457\n","Epoch: 20, Accuracy: 0.9459\n","Epoch: 21, Accuracy: 0.9464\n","Epoch: 22, Accuracy: 0.9472\n","Epoch: 23, Accuracy: 0.9474\n","Epoch: 24, Accuracy: 0.9477\n","Epoch: 25, Accuracy: 0.9475\n","Epoch: 26, Accuracy: 0.9476\n","Epoch: 27, Accuracy: 0.9476\n","Epoch: 28, Accuracy: 0.9478\n","Epoch: 29, Accuracy: 0.9475\n","Epoch: 30, Accuracy: 0.9478\n","Epoch: 31, Accuracy: 0.9479\n","Epoch: 32, Accuracy: 0.9482\n","Epoch: 33, Accuracy: 0.9484\n","Epoch: 34, Accuracy: 0.9487\n","Epoch: 35, Accuracy: 0.9488\n","Epoch: 36, Accuracy: 0.9486\n","Epoch: 37, Accuracy: 0.9489\n","Epoch: 38, Accuracy: 0.9492\n","Epoch: 39, Accuracy: 0.9493\n","Epoch: 40, Accuracy: 0.9496\n","Epoch: 41, Accuracy: 0.9498\n","Epoch: 42, Accuracy: 0.9503\n","Epoch: 43, Accuracy: 0.9502\n","Epoch: 44, Accuracy: 0.9508\n","Epoch: 45, Accuracy: 0.9512\n","Epoch: 46, Accuracy: 0.951\n","Epoch: 47, Accuracy: 0.9515\n","Epoch: 48, Accuracy: 0.9519\n","Epoch: 49, Accuracy: 0.9518\n","Epoch: 50, Accuracy: 0.9518\n","Epoch: 51, Accuracy: 0.9522\n","Epoch: 52, Accuracy: 0.9523\n","Epoch: 53, Accuracy: 0.9526\n","Epoch: 54, Accuracy: 0.9526\n","Epoch: 55, Accuracy: 0.9525\n","Epoch: 56, Accuracy: 0.9524\n","Epoch: 57, Accuracy: 0.9524\n","Epoch: 58, Accuracy: 0.9523\n","Epoch: 59, Accuracy: 0.9521\n","Epoch: 60, Accuracy: 0.9522\n","Epoch: 61, Accuracy: 0.952\n","Epoch: 62, Accuracy: 0.952\n","Epoch: 63, Accuracy: 0.952\n","Epoch: 64, Accuracy: 0.952\n","Epoch: 65, Accuracy: 0.9524\n","Epoch: 66, Accuracy: 0.9526\n","Epoch: 67, Accuracy: 0.9525\n","Epoch: 68, Accuracy: 0.9525\n","Epoch: 69, Accuracy: 0.9526\n","Epoch: 70, Accuracy: 0.9526\n","Epoch: 71, Accuracy: 0.9526\n","Epoch: 72, Accuracy: 0.9528\n","Epoch: 73, Accuracy: 0.9528\n","Epoch: 74, Accuracy: 0.9526\n","Epoch: 75, Accuracy: 0.9526\n","Epoch: 76, Accuracy: 0.9527\n","Epoch: 77, Accuracy: 0.9528\n","Epoch: 78, Accuracy: 0.9528\n","Epoch: 79, Accuracy: 0.9529\n","Epoch: 80, Accuracy: 0.9529\n","Epoch: 81, Accuracy: 0.9531\n","Epoch: 82, Accuracy: 0.953\n","Epoch: 83, Accuracy: 0.9532\n","Epoch: 84, Accuracy: 0.9534\n","Epoch: 85, Accuracy: 0.9531\n","Epoch: 86, Accuracy: 0.9532\n","Epoch: 87, Accuracy: 0.9533\n","Epoch: 88, Accuracy: 0.9535\n","Epoch: 89, Accuracy: 0.9535\n","Epoch: 90, Accuracy: 0.9534\n","Epoch: 91, Accuracy: 0.9537\n","Epoch: 92, Accuracy: 0.9536\n","Epoch: 93, Accuracy: 0.9538\n","Epoch: 94, Accuracy: 0.9538\n","Epoch: 95, Accuracy: 0.9539\n","Epoch: 96, Accuracy: 0.954\n","Epoch: 97, Accuracy: 0.954\n","Epoch: 98, Accuracy: 0.9537\n","Epoch: 99, Accuracy: 0.9535\n"]}]},{"cell_type":"markdown","metadata":{"id":"-cxoUOfUb2r5"},"source":["Nesterov's Momentum"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXXvVPnIcibZ","executionInfo":{"status":"ok","timestamp":1636756722110,"user_tz":-60,"elapsed":65355,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"e9b808ef-ccae-4a70-fcf9-4298a4fe98d7"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.m_b = [np.zeros_like(b) for b in self.biases]\n","        self.m_w = [np.zeros_like(w) for w in self.weights]\n","        self.mu = 0.9\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","\n","        # PROPER WAY?\n","        # for i in range(len(self.biases)):\n","        #   w_org, b_org = self.weights[i].copy(), self.biases[i].copy()\n","        #   self.weights[i] -= self.mu*self.m_w[i]\n","        #   self.biases[i] -= self.mu*self.m_b[i]\n","        #   nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        #   self.m_w[i] = self.mu*self.m_w[i] + (eta/len(x_batch))*nabla_w[i]\n","        #   self.m_b[i] = self.mu*self.m_b[i] + (eta/len(x_batch))*nabla_b[i]\n","        #   self.weights[i], self.biases[i] = w_org, b_org\n","\n","        self.weights = [w - self.mu*m for w, m in zip(self.weights, self.m_w)]\n","        self.biases = [b - self.mu*m for b, m in zip(self.biases, self.m_b)]\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        self.weights = [w + self.mu*m for w, m in zip(self.weights, self.m_w)]\n","        self.biases = [b + self.mu*m for b, m in zip(self.biases, self.m_b)]\n","\n","        self.m_w = [self.mu*m + (eta/len(x_batch))*nw for m, nw in zip(self.m_w, nabla_w)]\n","        self.m_b = [self.mu*m + (eta/len(x_batch))*nb for m, nb in zip(self.m_b, nabla_b)]\n","\n","        self.weights = [w - g for w, g in zip(self.weights, self.m_w)]\n","        self.biases = [b - g for b, g in zip(self.biases, self.m_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.1, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8711\n","Epoch: 1, Accuracy: 0.8973\n","Epoch: 2, Accuracy: 0.9094\n","Epoch: 3, Accuracy: 0.9157\n","Epoch: 4, Accuracy: 0.9211\n","Epoch: 5, Accuracy: 0.9262\n","Epoch: 6, Accuracy: 0.9294\n","Epoch: 7, Accuracy: 0.9318\n","Epoch: 8, Accuracy: 0.9331\n","Epoch: 9, Accuracy: 0.9336\n","Epoch: 10, Accuracy: 0.9354\n","Epoch: 11, Accuracy: 0.9361\n","Epoch: 12, Accuracy: 0.939\n","Epoch: 13, Accuracy: 0.9401\n","Epoch: 14, Accuracy: 0.9403\n","Epoch: 15, Accuracy: 0.9409\n","Epoch: 16, Accuracy: 0.9408\n","Epoch: 17, Accuracy: 0.941\n","Epoch: 18, Accuracy: 0.9413\n","Epoch: 19, Accuracy: 0.9415\n","Epoch: 20, Accuracy: 0.9428\n","Epoch: 21, Accuracy: 0.9438\n","Epoch: 22, Accuracy: 0.9443\n","Epoch: 23, Accuracy: 0.9451\n","Epoch: 24, Accuracy: 0.9454\n","Epoch: 25, Accuracy: 0.9461\n","Epoch: 26, Accuracy: 0.9467\n","Epoch: 27, Accuracy: 0.9467\n","Epoch: 28, Accuracy: 0.9467\n","Epoch: 29, Accuracy: 0.947\n","Epoch: 30, Accuracy: 0.9473\n","Epoch: 31, Accuracy: 0.9474\n","Epoch: 32, Accuracy: 0.9475\n","Epoch: 33, Accuracy: 0.9473\n","Epoch: 34, Accuracy: 0.9478\n","Epoch: 35, Accuracy: 0.9479\n","Epoch: 36, Accuracy: 0.9481\n","Epoch: 37, Accuracy: 0.9488\n","Epoch: 38, Accuracy: 0.9484\n","Epoch: 39, Accuracy: 0.9485\n","Epoch: 40, Accuracy: 0.9483\n","Epoch: 41, Accuracy: 0.9484\n","Epoch: 42, Accuracy: 0.9486\n","Epoch: 43, Accuracy: 0.9486\n","Epoch: 44, Accuracy: 0.9489\n","Epoch: 45, Accuracy: 0.9486\n","Epoch: 46, Accuracy: 0.9486\n","Epoch: 47, Accuracy: 0.9492\n","Epoch: 48, Accuracy: 0.9492\n","Epoch: 49, Accuracy: 0.9495\n","Epoch: 50, Accuracy: 0.9494\n","Epoch: 51, Accuracy: 0.9495\n","Epoch: 52, Accuracy: 0.9497\n","Epoch: 53, Accuracy: 0.9496\n","Epoch: 54, Accuracy: 0.9496\n","Epoch: 55, Accuracy: 0.95\n","Epoch: 56, Accuracy: 0.9501\n","Epoch: 57, Accuracy: 0.9503\n","Epoch: 58, Accuracy: 0.9508\n","Epoch: 59, Accuracy: 0.9513\n","Epoch: 60, Accuracy: 0.9513\n","Epoch: 61, Accuracy: 0.9513\n","Epoch: 62, Accuracy: 0.9515\n","Epoch: 63, Accuracy: 0.9514\n","Epoch: 64, Accuracy: 0.9515\n","Epoch: 65, Accuracy: 0.9515\n","Epoch: 66, Accuracy: 0.9512\n","Epoch: 67, Accuracy: 0.9515\n","Epoch: 68, Accuracy: 0.9517\n","Epoch: 69, Accuracy: 0.9518\n","Epoch: 70, Accuracy: 0.9519\n","Epoch: 71, Accuracy: 0.9519\n","Epoch: 72, Accuracy: 0.9524\n","Epoch: 73, Accuracy: 0.9524\n","Epoch: 74, Accuracy: 0.9524\n","Epoch: 75, Accuracy: 0.9524\n","Epoch: 76, Accuracy: 0.9526\n","Epoch: 77, Accuracy: 0.9525\n","Epoch: 78, Accuracy: 0.9526\n","Epoch: 79, Accuracy: 0.9527\n","Epoch: 80, Accuracy: 0.9524\n","Epoch: 81, Accuracy: 0.9525\n","Epoch: 82, Accuracy: 0.9525\n","Epoch: 83, Accuracy: 0.9521\n","Epoch: 84, Accuracy: 0.9519\n","Epoch: 85, Accuracy: 0.9522\n","Epoch: 86, Accuracy: 0.9522\n","Epoch: 87, Accuracy: 0.9525\n","Epoch: 88, Accuracy: 0.9525\n","Epoch: 89, Accuracy: 0.9526\n","Epoch: 90, Accuracy: 0.9527\n","Epoch: 91, Accuracy: 0.9525\n","Epoch: 92, Accuracy: 0.9526\n","Epoch: 93, Accuracy: 0.9524\n","Epoch: 94, Accuracy: 0.9522\n","Epoch: 95, Accuracy: 0.9521\n","Epoch: 96, Accuracy: 0.9518\n","Epoch: 97, Accuracy: 0.9519\n","Epoch: 98, Accuracy: 0.9519\n","Epoch: 99, Accuracy: 0.9519\n"]}]},{"cell_type":"markdown","metadata":{"id":"8y0YnpskQKWp"},"source":["RMSProp"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6XeOAVjQMVV","executionInfo":{"status":"ok","timestamp":1636736929141,"user_tz":-60,"elapsed":71544,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"ac6df835-4703-4812-c016-059e9e358a7d"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.m_b = [np.zeros_like(w) for w in self.biases]\n","        self.m_w = [np.zeros_like(w) for w in self.weights]\n","        self.mu = 0.9\n","        self.eps = 1e-8\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        \n","        self.m_b = [self.mu * m + (1 - self.mu) * (nb/len(x_batch))**2 \\\n","                    for m, nb in zip(self.m_b, nabla_b)]\n","        self.biases = [b - (eta / np.sqrt(m + self.eps)) * (nb/len(x_batch)) \\\n","                       for b, nb, m in zip(self.biases, nabla_b, self.m_b)]\n","        \n","        self.m_w = [self.mu * m + (1 - self.mu) * (nw/len(x_batch))**2 \\\n","                    for m, nw in zip(self.m_w, nabla_w)]\n","        self.weights = [b - (eta/np.sqrt(m + self.eps)) * (nw/len(x_batch)) \\\n","                        for b, nw, m in zip(self.weights, nabla_w, self.m_w)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.01, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.9045\n","Epoch: 1, Accuracy: 0.9196\n","Epoch: 2, Accuracy: 0.9298\n","Epoch: 3, Accuracy: 0.9345\n","Epoch: 4, Accuracy: 0.938\n","Epoch: 5, Accuracy: 0.9418\n","Epoch: 6, Accuracy: 0.9454\n","Epoch: 7, Accuracy: 0.9465\n","Epoch: 8, Accuracy: 0.9446\n","Epoch: 9, Accuracy: 0.9466\n","Epoch: 10, Accuracy: 0.9477\n","Epoch: 11, Accuracy: 0.9467\n","Epoch: 12, Accuracy: 0.9472\n","Epoch: 13, Accuracy: 0.9462\n","Epoch: 14, Accuracy: 0.9452\n","Epoch: 15, Accuracy: 0.9461\n","Epoch: 16, Accuracy: 0.9462\n","Epoch: 17, Accuracy: 0.9454\n","Epoch: 18, Accuracy: 0.9468\n","Epoch: 19, Accuracy: 0.947\n","Epoch: 20, Accuracy: 0.9463\n","Epoch: 21, Accuracy: 0.947\n","Epoch: 22, Accuracy: 0.9476\n","Epoch: 23, Accuracy: 0.9477\n","Epoch: 24, Accuracy: 0.946\n","Epoch: 25, Accuracy: 0.9464\n","Epoch: 26, Accuracy: 0.9479\n","Epoch: 27, Accuracy: 0.95\n","Epoch: 28, Accuracy: 0.9495\n","Epoch: 29, Accuracy: 0.9501\n","Epoch: 30, Accuracy: 0.9488\n","Epoch: 31, Accuracy: 0.9485\n","Epoch: 32, Accuracy: 0.9475\n","Epoch: 33, Accuracy: 0.9472\n","Epoch: 34, Accuracy: 0.9484\n","Epoch: 35, Accuracy: 0.948\n","Epoch: 36, Accuracy: 0.9471\n","Epoch: 37, Accuracy: 0.9489\n","Epoch: 38, Accuracy: 0.9482\n","Epoch: 39, Accuracy: 0.9483\n","Epoch: 40, Accuracy: 0.9488\n","Epoch: 41, Accuracy: 0.949\n","Epoch: 42, Accuracy: 0.9473\n","Epoch: 43, Accuracy: 0.9486\n","Epoch: 44, Accuracy: 0.9479\n","Epoch: 45, Accuracy: 0.9494\n","Epoch: 46, Accuracy: 0.949\n","Epoch: 47, Accuracy: 0.9485\n","Epoch: 48, Accuracy: 0.9475\n","Epoch: 49, Accuracy: 0.9478\n","Epoch: 50, Accuracy: 0.9479\n","Epoch: 51, Accuracy: 0.9499\n","Epoch: 52, Accuracy: 0.9495\n","Epoch: 53, Accuracy: 0.9489\n","Epoch: 54, Accuracy: 0.9496\n","Epoch: 55, Accuracy: 0.9495\n","Epoch: 56, Accuracy: 0.9489\n","Epoch: 57, Accuracy: 0.9503\n","Epoch: 58, Accuracy: 0.9502\n","Epoch: 59, Accuracy: 0.951\n","Epoch: 60, Accuracy: 0.9499\n","Epoch: 61, Accuracy: 0.9495\n","Epoch: 62, Accuracy: 0.9482\n","Epoch: 63, Accuracy: 0.9498\n","Epoch: 64, Accuracy: 0.9499\n","Epoch: 65, Accuracy: 0.9493\n","Epoch: 66, Accuracy: 0.9499\n","Epoch: 67, Accuracy: 0.9474\n","Epoch: 68, Accuracy: 0.9469\n","Epoch: 69, Accuracy: 0.949\n","Epoch: 70, Accuracy: 0.9481\n","Epoch: 71, Accuracy: 0.9478\n","Epoch: 72, Accuracy: 0.9478\n","Epoch: 73, Accuracy: 0.9472\n","Epoch: 74, Accuracy: 0.9462\n","Epoch: 75, Accuracy: 0.947\n","Epoch: 76, Accuracy: 0.9468\n","Epoch: 77, Accuracy: 0.9461\n","Epoch: 78, Accuracy: 0.9473\n","Epoch: 79, Accuracy: 0.9457\n","Epoch: 80, Accuracy: 0.9463\n","Epoch: 81, Accuracy: 0.9458\n","Epoch: 82, Accuracy: 0.9469\n","Epoch: 83, Accuracy: 0.9461\n","Epoch: 84, Accuracy: 0.9475\n","Epoch: 85, Accuracy: 0.9484\n","Epoch: 86, Accuracy: 0.9473\n","Epoch: 87, Accuracy: 0.9475\n","Epoch: 88, Accuracy: 0.9465\n","Epoch: 89, Accuracy: 0.9454\n","Epoch: 90, Accuracy: 0.9456\n","Epoch: 91, Accuracy: 0.9456\n","Epoch: 92, Accuracy: 0.9455\n","Epoch: 93, Accuracy: 0.9461\n","Epoch: 94, Accuracy: 0.9471\n","Epoch: 95, Accuracy: 0.9463\n","Epoch: 96, Accuracy: 0.9455\n","Epoch: 97, Accuracy: 0.9462\n","Epoch: 98, Accuracy: 0.9456\n","Epoch: 99, Accuracy: 0.947\n"]}]},{"cell_type":"markdown","metadata":{"id":"d0DVmobtTGeT"},"source":["Adam"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnuu090oTHzu","executionInfo":{"status":"ok","timestamp":1636738754859,"user_tz":-60,"elapsed":87623,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"85eb51ee-f71c-410a-b4cf-7dded2175efd"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.m_w = [np.zeros_like(w) for w in self.weights]\n","        self.g_w = [np.zeros_like(w) for w in self.weights]\n","        self.m_b = [np.zeros_like(w) for w in self.biases]\n","        self.g_b = [np.zeros_like(w) for w in self.biases]\n","        self.beta2 = 0.999\n","        self.beta1 = 0.9\n","        self.eps = 1e-8\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta, epoch):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","\n","        self.m_w = [self.beta1 * m + (1 - self.beta1) * (nw/len(x_batch))\\\n","                  for m, nw in zip(self.m_w, nabla_w)]\n","        self.g_w = [self.beta2 * g + (1 - self.beta2) * (nw/len(x_batch))**2\\\n","                  for g, nw in zip(self.g_w, nabla_w)]\n","        self.weights = [w - (eta / (np.sqrt(g/(1 - self.beta2 ** epoch)) + self.eps)) * m/(1 - self.beta1 ** epoch)\\\n","                        for w, m, g in zip(self.weights, self.m_w, self.g_w)]\n","\n","        self.m_b = [self.beta1 * m + (1 - self.beta1) * (nw/len(x_batch))\\\n","                  for m, nw in zip(self.m_b, nabla_b)]\n","        self.g_b = [self.beta2 * g + (1 - self.beta2) * (nw/len(x_batch))**2\\\n","                  for g, nw in zip(self.g_b, nabla_b)]\n","        self.biases = [w - (eta / (np.sqrt(g/(1 - self.beta2 ** epoch)) + self.eps)) * m/(1 - self.beta1 ** epoch)\\\n","                        for w, m, g in zip(self.biases, self.m_b, self.g_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            idx = np.random.permutation(x_train.shape[0])\n","            x_train = x_train[idx]\n","            y_train = y_train[idx]\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta, j+1)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.1, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.9198\n","Epoch: 1, Accuracy: 0.9325\n","Epoch: 2, Accuracy: 0.9373\n","Epoch: 3, Accuracy: 0.9409\n","Epoch: 4, Accuracy: 0.9424\n","Epoch: 5, Accuracy: 0.9457\n","Epoch: 6, Accuracy: 0.9468\n","Epoch: 7, Accuracy: 0.944\n","Epoch: 8, Accuracy: 0.9484\n","Epoch: 9, Accuracy: 0.9469\n","Epoch: 10, Accuracy: 0.9487\n","Epoch: 11, Accuracy: 0.9503\n","Epoch: 12, Accuracy: 0.9498\n","Epoch: 13, Accuracy: 0.9482\n","Epoch: 14, Accuracy: 0.9515\n","Epoch: 15, Accuracy: 0.9503\n","Epoch: 16, Accuracy: 0.9501\n","Epoch: 17, Accuracy: 0.9493\n","Epoch: 18, Accuracy: 0.9516\n","Epoch: 19, Accuracy: 0.9493\n","Epoch: 20, Accuracy: 0.9473\n","Epoch: 21, Accuracy: 0.9521\n","Epoch: 22, Accuracy: 0.9533\n","Epoch: 23, Accuracy: 0.9494\n","Epoch: 24, Accuracy: 0.9501\n","Epoch: 25, Accuracy: 0.9514\n","Epoch: 26, Accuracy: 0.9502\n","Epoch: 27, Accuracy: 0.9508\n","Epoch: 28, Accuracy: 0.9516\n","Epoch: 29, Accuracy: 0.9484\n","Epoch: 30, Accuracy: 0.9499\n","Epoch: 31, Accuracy: 0.9509\n","Epoch: 32, Accuracy: 0.9524\n","Epoch: 33, Accuracy: 0.9533\n","Epoch: 34, Accuracy: 0.9498\n","Epoch: 35, Accuracy: 0.9526\n","Epoch: 36, Accuracy: 0.9505\n","Epoch: 37, Accuracy: 0.9509\n","Epoch: 38, Accuracy: 0.9501\n","Epoch: 39, Accuracy: 0.9516\n","Epoch: 40, Accuracy: 0.95\n","Epoch: 41, Accuracy: 0.9547\n","Epoch: 42, Accuracy: 0.9485\n","Epoch: 43, Accuracy: 0.9523\n","Epoch: 44, Accuracy: 0.9502\n","Epoch: 45, Accuracy: 0.9495\n","Epoch: 46, Accuracy: 0.9523\n","Epoch: 47, Accuracy: 0.9504\n","Epoch: 48, Accuracy: 0.9494\n","Epoch: 49, Accuracy: 0.9532\n","Epoch: 50, Accuracy: 0.9537\n","Epoch: 51, Accuracy: 0.9524\n","Epoch: 52, Accuracy: 0.9525\n","Epoch: 53, Accuracy: 0.9496\n","Epoch: 54, Accuracy: 0.9481\n","Epoch: 55, Accuracy: 0.9518\n","Epoch: 56, Accuracy: 0.9504\n","Epoch: 57, Accuracy: 0.9508\n","Epoch: 58, Accuracy: 0.9539\n","Epoch: 59, Accuracy: 0.9482\n","Epoch: 60, Accuracy: 0.9514\n","Epoch: 61, Accuracy: 0.9513\n","Epoch: 62, Accuracy: 0.9508\n","Epoch: 63, Accuracy: 0.949\n","Epoch: 64, Accuracy: 0.9549\n","Epoch: 65, Accuracy: 0.9532\n","Epoch: 66, Accuracy: 0.9512\n","Epoch: 67, Accuracy: 0.9495\n","Epoch: 68, Accuracy: 0.9531\n","Epoch: 69, Accuracy: 0.9474\n","Epoch: 70, Accuracy: 0.9518\n","Epoch: 71, Accuracy: 0.9491\n","Epoch: 72, Accuracy: 0.9526\n","Epoch: 73, Accuracy: 0.9484\n","Epoch: 74, Accuracy: 0.9485\n","Epoch: 75, Accuracy: 0.9514\n","Epoch: 76, Accuracy: 0.9508\n","Epoch: 77, Accuracy: 0.9515\n","Epoch: 78, Accuracy: 0.9471\n","Epoch: 79, Accuracy: 0.949\n","Epoch: 80, Accuracy: 0.9492\n","Epoch: 81, Accuracy: 0.9506\n","Epoch: 82, Accuracy: 0.9517\n","Epoch: 83, Accuracy: 0.9506\n","Epoch: 84, Accuracy: 0.9519\n","Epoch: 85, Accuracy: 0.9494\n","Epoch: 86, Accuracy: 0.9507\n","Epoch: 87, Accuracy: 0.9491\n","Epoch: 88, Accuracy: 0.9526\n","Epoch: 89, Accuracy: 0.952\n","Epoch: 90, Accuracy: 0.9471\n","Epoch: 91, Accuracy: 0.951\n","Epoch: 92, Accuracy: 0.9525\n","Epoch: 93, Accuracy: 0.9538\n","Epoch: 94, Accuracy: 0.948\n","Epoch: 95, Accuracy: 0.9514\n","Epoch: 96, Accuracy: 0.9511\n","Epoch: 97, Accuracy: 0.949\n","Epoch: 98, Accuracy: 0.9482\n","Epoch: 99, Accuracy: 0.9465\n"]}]},{"cell_type":"markdown","metadata":{"id":"Iy6WbuEVJ5es"},"source":["Dropout"]},{"cell_type":"code","metadata":{"id":"0IdUHNNtJ8gK"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        self.weights = [w-(eta/len(x_batch))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        dropout_mask = np.concatenate([np.random])\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=1.0, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sm9nZcB4Fq-"},"source":["Augmentation\n","\n","Swap adjacent rows and cols"]},{"cell_type":"code","metadata":{"id":"yNoQOudbU97k"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veKLAPloU_qn"},"source":["torch.distributions.Normal(0, torch.tensor([1.0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8IdeLRk4Vx1"},"source":["def aug(x, p):\n","  for el in x:\n","    if np.random.rand() < p:\n","      nb = np.random.randint(0, el.shape[0]-1)\n","      el[[nb,nb+1]] = el[[nb+1, nb]]\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esVCdQDa4N_j","executionInfo":{"status":"ok","timestamp":1636817202140,"user_tz":-60,"elapsed":187231,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"6cd1ab3f-8f7e-4c61-fb64-e9a911f48bda"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(aug(x_batch.copy(), 0.3).T, y_batch.T)\n","        self.weights = [w-(eta/len(x_batch))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=1.0, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8553\n","Epoch: 1, Accuracy: 0.885\n","Epoch: 2, Accuracy: 0.8998\n","Epoch: 3, Accuracy: 0.9073\n","Epoch: 4, Accuracy: 0.9115\n","Epoch: 5, Accuracy: 0.9152\n","Epoch: 6, Accuracy: 0.9187\n","Epoch: 7, Accuracy: 0.9219\n","Epoch: 8, Accuracy: 0.9238\n","Epoch: 9, Accuracy: 0.9257\n","Epoch: 10, Accuracy: 0.9284\n","Epoch: 11, Accuracy: 0.9305\n","Epoch: 12, Accuracy: 0.9323\n","Epoch: 13, Accuracy: 0.9341\n","Epoch: 14, Accuracy: 0.9352\n","Epoch: 15, Accuracy: 0.9358\n","Epoch: 16, Accuracy: 0.9368\n","Epoch: 17, Accuracy: 0.9371\n","Epoch: 18, Accuracy: 0.9386\n","Epoch: 19, Accuracy: 0.9391\n","Epoch: 20, Accuracy: 0.9392\n","Epoch: 21, Accuracy: 0.9397\n","Epoch: 22, Accuracy: 0.9405\n","Epoch: 23, Accuracy: 0.941\n","Epoch: 24, Accuracy: 0.9408\n","Epoch: 25, Accuracy: 0.9414\n","Epoch: 26, Accuracy: 0.9415\n","Epoch: 27, Accuracy: 0.9422\n","Epoch: 28, Accuracy: 0.9428\n","Epoch: 29, Accuracy: 0.9427\n","Epoch: 30, Accuracy: 0.9435\n","Epoch: 31, Accuracy: 0.9431\n","Epoch: 32, Accuracy: 0.9436\n","Epoch: 33, Accuracy: 0.9432\n","Epoch: 34, Accuracy: 0.944\n","Epoch: 35, Accuracy: 0.9437\n","Epoch: 36, Accuracy: 0.9443\n","Epoch: 37, Accuracy: 0.9444\n","Epoch: 38, Accuracy: 0.9441\n","Epoch: 39, Accuracy: 0.9445\n","Epoch: 40, Accuracy: 0.9444\n","Epoch: 41, Accuracy: 0.9448\n","Epoch: 42, Accuracy: 0.9455\n","Epoch: 43, Accuracy: 0.9455\n","Epoch: 44, Accuracy: 0.9452\n","Epoch: 45, Accuracy: 0.9455\n","Epoch: 46, Accuracy: 0.9446\n","Epoch: 47, Accuracy: 0.9451\n","Epoch: 48, Accuracy: 0.945\n","Epoch: 49, Accuracy: 0.945\n","Epoch: 50, Accuracy: 0.9454\n","Epoch: 51, Accuracy: 0.9452\n","Epoch: 52, Accuracy: 0.9455\n","Epoch: 53, Accuracy: 0.9454\n","Epoch: 54, Accuracy: 0.945\n","Epoch: 55, Accuracy: 0.9451\n","Epoch: 56, Accuracy: 0.9457\n","Epoch: 57, Accuracy: 0.9456\n","Epoch: 58, Accuracy: 0.9459\n","Epoch: 59, Accuracy: 0.9459\n","Epoch: 60, Accuracy: 0.9459\n","Epoch: 61, Accuracy: 0.9457\n","Epoch: 62, Accuracy: 0.946\n","Epoch: 63, Accuracy: 0.9457\n","Epoch: 64, Accuracy: 0.9454\n","Epoch: 65, Accuracy: 0.9451\n","Epoch: 66, Accuracy: 0.945\n","Epoch: 67, Accuracy: 0.9448\n","Epoch: 68, Accuracy: 0.9446\n","Epoch: 69, Accuracy: 0.9444\n","Epoch: 70, Accuracy: 0.9444\n","Epoch: 71, Accuracy: 0.9447\n","Epoch: 72, Accuracy: 0.9441\n","Epoch: 73, Accuracy: 0.9441\n","Epoch: 74, Accuracy: 0.9443\n","Epoch: 75, Accuracy: 0.9443\n","Epoch: 76, Accuracy: 0.9445\n","Epoch: 77, Accuracy: 0.9441\n","Epoch: 78, Accuracy: 0.9448\n","Epoch: 79, Accuracy: 0.9452\n","Epoch: 80, Accuracy: 0.9448\n","Epoch: 81, Accuracy: 0.9442\n","Epoch: 82, Accuracy: 0.9445\n","Epoch: 83, Accuracy: 0.9444\n","Epoch: 84, Accuracy: 0.945\n","Epoch: 85, Accuracy: 0.9446\n","Epoch: 86, Accuracy: 0.9448\n","Epoch: 87, Accuracy: 0.9446\n","Epoch: 88, Accuracy: 0.9444\n","Epoch: 89, Accuracy: 0.9442\n","Epoch: 90, Accuracy: 0.944\n","Epoch: 91, Accuracy: 0.9443\n","Epoch: 92, Accuracy: 0.9446\n","Epoch: 93, Accuracy: 0.9451\n","Epoch: 94, Accuracy: 0.9448\n","Epoch: 95, Accuracy: 0.9448\n","Epoch: 96, Accuracy: 0.9449\n","Epoch: 97, Accuracy: 0.9442\n","Epoch: 98, Accuracy: 0.9444\n","Epoch: 99, Accuracy: 0.9444\n"]}]},{"cell_type":"markdown","metadata":{"id":"vM0sY2ldrOQ2"},"source":["Softmax"]},{"cell_type":"code","metadata":{"id":"t0qRl40YP6I8"},"source":["def stratified_split(y_train, bs):\n","  n_class = np.unique(y_train).shape[0]\n","  splitted_y = [(y_train == i).nonzero()[0] for i in np.unique(y_train)]\n","  for _ in range(y_train.shape[0] // bs):\n","    batch_idx = []\n","    for i in range(n_class):\n","      sample = splitted_y[i][:bs // n_class]\n","      splitted_y[i] = np.delete(splitted_y[i], np.where(splitted_y[i] == sample))\n","      batch_idx += sample.tolist()\n","    yield np.random.permutation(np.array(batch_idx))\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZ6vDIglrQZJ","executionInfo":{"status":"ok","timestamp":1636741600030,"user_tz":-60,"elapsed":65344,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"a50a5b4c-1e15-48e8-ce3d-20f2021de575"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return h\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        self.weights = [w-(eta/len(x_batch))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(h, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (softmax(output_activations)-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            idx = np.random.permutation(x_train.shape[0])\n","            x_train = x_train[idx]\n","            y_train = y_train[idx]\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","            # for batch_idx in stratified_split(y_train, mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","                # self.update_mini_batch(x_train[batch_idx], y_train[batch_idx], eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=1.0, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8602\n","Epoch: 1, Accuracy: 0.8876\n","Epoch: 2, Accuracy: 0.9001\n","Epoch: 3, Accuracy: 0.9094\n","Epoch: 4, Accuracy: 0.915\n","Epoch: 5, Accuracy: 0.9175\n","Epoch: 6, Accuracy: 0.9228\n","Epoch: 7, Accuracy: 0.9247\n","Epoch: 8, Accuracy: 0.927\n","Epoch: 9, Accuracy: 0.9308\n","Epoch: 10, Accuracy: 0.9322\n","Epoch: 11, Accuracy: 0.9347\n","Epoch: 12, Accuracy: 0.934\n","Epoch: 13, Accuracy: 0.9353\n","Epoch: 14, Accuracy: 0.9367\n","Epoch: 15, Accuracy: 0.9399\n","Epoch: 16, Accuracy: 0.9405\n","Epoch: 17, Accuracy: 0.9417\n","Epoch: 18, Accuracy: 0.9407\n","Epoch: 19, Accuracy: 0.9436\n","Epoch: 20, Accuracy: 0.9424\n","Epoch: 21, Accuracy: 0.9427\n","Epoch: 22, Accuracy: 0.9434\n","Epoch: 23, Accuracy: 0.9433\n","Epoch: 24, Accuracy: 0.9447\n","Epoch: 25, Accuracy: 0.9457\n","Epoch: 26, Accuracy: 0.9458\n","Epoch: 27, Accuracy: 0.9468\n","Epoch: 28, Accuracy: 0.9468\n","Epoch: 29, Accuracy: 0.9462\n","Epoch: 30, Accuracy: 0.9469\n","Epoch: 31, Accuracy: 0.9477\n","Epoch: 32, Accuracy: 0.9465\n","Epoch: 33, Accuracy: 0.9489\n","Epoch: 34, Accuracy: 0.9472\n","Epoch: 35, Accuracy: 0.9492\n","Epoch: 36, Accuracy: 0.9471\n","Epoch: 37, Accuracy: 0.9479\n","Epoch: 38, Accuracy: 0.9478\n","Epoch: 39, Accuracy: 0.9487\n","Epoch: 40, Accuracy: 0.9491\n","Epoch: 41, Accuracy: 0.9493\n","Epoch: 42, Accuracy: 0.95\n","Epoch: 43, Accuracy: 0.9496\n","Epoch: 44, Accuracy: 0.9487\n","Epoch: 45, Accuracy: 0.948\n","Epoch: 46, Accuracy: 0.9494\n","Epoch: 47, Accuracy: 0.9494\n","Epoch: 48, Accuracy: 0.9508\n","Epoch: 49, Accuracy: 0.95\n","Epoch: 50, Accuracy: 0.9496\n","Epoch: 51, Accuracy: 0.9507\n","Epoch: 52, Accuracy: 0.9508\n","Epoch: 53, Accuracy: 0.949\n","Epoch: 54, Accuracy: 0.9507\n","Epoch: 55, Accuracy: 0.9515\n","Epoch: 56, Accuracy: 0.9497\n","Epoch: 57, Accuracy: 0.9514\n","Epoch: 58, Accuracy: 0.9516\n","Epoch: 59, Accuracy: 0.9508\n","Epoch: 60, Accuracy: 0.9509\n","Epoch: 61, Accuracy: 0.951\n","Epoch: 62, Accuracy: 0.9504\n","Epoch: 63, Accuracy: 0.9506\n","Epoch: 64, Accuracy: 0.952\n","Epoch: 65, Accuracy: 0.951\n","Epoch: 66, Accuracy: 0.9509\n","Epoch: 67, Accuracy: 0.95\n","Epoch: 68, Accuracy: 0.9512\n","Epoch: 69, Accuracy: 0.9511\n","Epoch: 70, Accuracy: 0.952\n","Epoch: 71, Accuracy: 0.9522\n","Epoch: 72, Accuracy: 0.9512\n","Epoch: 73, Accuracy: 0.9523\n","Epoch: 74, Accuracy: 0.9511\n","Epoch: 75, Accuracy: 0.9518\n","Epoch: 76, Accuracy: 0.9517\n","Epoch: 77, Accuracy: 0.9505\n","Epoch: 78, Accuracy: 0.9511\n","Epoch: 79, Accuracy: 0.9517\n","Epoch: 80, Accuracy: 0.9528\n","Epoch: 81, Accuracy: 0.9521\n","Epoch: 82, Accuracy: 0.9516\n","Epoch: 83, Accuracy: 0.951\n","Epoch: 84, Accuracy: 0.9516\n","Epoch: 85, Accuracy: 0.9512\n","Epoch: 86, Accuracy: 0.952\n","Epoch: 87, Accuracy: 0.9517\n","Epoch: 88, Accuracy: 0.95\n","Epoch: 89, Accuracy: 0.9531\n","Epoch: 90, Accuracy: 0.9501\n","Epoch: 91, Accuracy: 0.9504\n","Epoch: 92, Accuracy: 0.9515\n","Epoch: 93, Accuracy: 0.9498\n","Epoch: 94, Accuracy: 0.9506\n","Epoch: 95, Accuracy: 0.9518\n","Epoch: 96, Accuracy: 0.9507\n","Epoch: 97, Accuracy: 0.9499\n","Epoch: 98, Accuracy: 0.9513\n","Epoch: 99, Accuracy: 0.9503\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHMiTOVRVTCC","executionInfo":{"status":"ok","timestamp":1636741190956,"user_tz":-60,"elapsed":323,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"1c516f74-861f-4aed-de19-8e1b20969843"},"source":["np.random.permutation(np.arange(2,8))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 7, 2, 6, 4, 5])"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EKwK4XEXnX3","executionInfo":{"status":"ok","timestamp":1636741057436,"user_tz":-60,"elapsed":420,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"4fcb508a-ea64-4dd3-9416-4d43a0b1f527"},"source":["y_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 10)"]},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","metadata":{"id":"kqxNoZVTog-q"},"source":["Softmax + ReLU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"P3YWqFzXojXY","executionInfo":{"status":"error","timestamp":1636925214245,"user_tz":-60,"elapsed":6896,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"588d4775-1733-44c5-b526-1393a081f454"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = relu(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        return softmax(h)\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        self.weights = [w-(eta/len(x_batch))*nw \n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    \n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h1 = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = relu(h1)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        a2 = softmax(h)\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph2 = self.cost_derivative(a2, y_batch)\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","        \n","        ph1 = self.weights[1].T @ ph2\n","        ph1[h1 < 0] = 0\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.5, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8175\n","Epoch: 1, Accuracy: 0.8564\n","Epoch: 2, Accuracy: 0.8727\n","Epoch: 3, Accuracy: 0.8852\n","Epoch: 4, Accuracy: 0.8885\n","Epoch: 5, Accuracy: 0.8936\n","Epoch: 6, Accuracy: 0.8983\n","Epoch: 7, Accuracy: 0.9027\n","Epoch: 8, Accuracy: 0.9071\n","Epoch: 9, Accuracy: 0.9135\n","Epoch: 10, Accuracy: 0.9154\n","Epoch: 11, Accuracy: 0.9178\n","Epoch: 12, Accuracy: 0.9187\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-32bd0c4ee7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-32bd0c4ee7a7>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mx_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0my_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {0}, Accuracy: {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-32bd0c4ee7a7>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, x_batch, y_batch, eta)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# The gradient is computed for a mini_batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# eta is the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         self.weights = [w-(eta/len(x_batch))*nw \n\u001b[1;32m     25\u001b[0m                         for w, nw in zip(self.weights, nabla_w)]\n","\u001b[0;32m<ipython-input-10-32bd0c4ee7a7>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mph1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mph1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mph1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"EsNGPAsZFi57"},"source":["Weight Decay"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXAjANxShj_x","executionInfo":{"status":"ok","timestamp":1636925536194,"user_tz":-60,"elapsed":139712,"user":{"displayName":"Bartłomiej Krzepkowski","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14471650214675903305"}},"outputId":"f1d7f7a8-5df6-42b7-ee71-c44a3930b364"},"source":["class Network(object):\n","    def __init__(self, sizes):\n","        # initialize biases and weights with random normal distr.\n","        # weights are indexed by target node first\n","        self.num_layers = len(sizes)\n","        self.sizes = sizes\n","        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n","        self.weights = [np.random.randn(y, x) \n","                        for x, y in zip(sizes[:-1], sizes[1:])]\n","        self.alpha = 0.0001\n","\n","    def feedforward(self, a):\n","        # Run the network on a batch\n","        a = a.T\n","        h = np.dot(self.weights[0], a) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        a2 = sigmoid(h)\n","        h = np.dot(self.weights[2], a2) + self.biases[2]\n","        return softmax(h)\n","    \n","    def update_mini_batch(self, x_batch, y_batch, eta):\n","        # Update networks weights and biases by applying a single step\n","        # of gradient descent using backpropagation to compute the gradient.\n","        # The gradient is computed for a mini_batch.\n","        # eta is the learning rate\n","        nabla_b, nabla_w = self.backprop(x_batch.T, y_batch.T)\n","        self.weights = [w-(eta/len(x_batch))*nw\n","                        for w, nw in zip(self.weights, nabla_w)]\n","        self.biases = [b-(eta/len(x_batch))*nb \n","                       for b, nb in zip(self.biases, nabla_b)]\n","\n","    \n","\n","    def backprop(self, x_batch, y_batch):\n","        # For a single input (x,y) return a tuple of lists.\n","        # First contains gradients over biases, second over weights.\n","        \n","        # First initialize the list of gradient arrays\n","        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","\n","        h = np.dot(self.weights[0], x_batch) + self.biases[0]\n","        a1 = sigmoid(h)\n","        h = np.dot(self.weights[1], a1) + self.biases[1]\n","        a2 = sigmoid(h)\n","        # a1 = relu(h)\n","        h = np.dot(self.weights[2], a2) + self.biases[2]\n","        a3 = softmax(h)\n","        \n","        # Now go backward from the final cost applying backpropagation\n","        ph3 = self.cost_derivative(a3, y_batch)\n","        delta_nabla_b[2] += ph3.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[2] += ph3 @ a2.T\n","        \n","        ph2 = self.weights[2].T @ ph3 * a2 * (1 - a2)\n","        # ph1[a1 < 0] = 0\n","        delta_nabla_b[1] += ph2.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[1] += ph2 @ a1.T\n","\n","        ph1 = self.weights[1].T @ ph2 * a1 * (1 - a1)\n","        # ph1[a1 < 0] = 0\n","        delta_nabla_b[0] += ph1.sum(axis=1)[:, np.newaxis]\n","        delta_nabla_w[0] += ph1 @ x_batch.T\n","\n","        return (delta_nabla_b, delta_nabla_w)\n","        \n","\n","    def evaluate(self, test_data):\n","        # Count the number of correct answers for test_data\n","        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n","        corr = np.argmax(test_data[1],axis=1).T\n","        return np.mean(pred==corr)\n","    \n","    def cost_derivative(self, output_activations, y):\n","        return (output_activations-y) \n","    \n","    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n","        x_train, y_train = training_data\n","        if test_data:\n","            x_test, y_test = test_data\n","        for j in range(epochs):\n","            for i in range(x_train.shape[0] // mini_batch_size):\n","                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n","                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n","            if test_data:\n","                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n","            else:\n","                print(\"Epoch: {0}\".format(j))\n","\n","\n","network = Network([784,100,30,10])\n","network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=0.5, test_data=(x_test, y_test))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Accuracy: 0.8231\n","Epoch: 1, Accuracy: 0.8727\n","Epoch: 2, Accuracy: 0.8913\n","Epoch: 3, Accuracy: 0.9027\n","Epoch: 4, Accuracy: 0.9094\n","Epoch: 5, Accuracy: 0.9156\n","Epoch: 6, Accuracy: 0.92\n","Epoch: 7, Accuracy: 0.9239\n","Epoch: 8, Accuracy: 0.9266\n","Epoch: 9, Accuracy: 0.929\n","Epoch: 10, Accuracy: 0.931\n","Epoch: 11, Accuracy: 0.9324\n","Epoch: 12, Accuracy: 0.9338\n","Epoch: 13, Accuracy: 0.9346\n","Epoch: 14, Accuracy: 0.936\n","Epoch: 15, Accuracy: 0.9372\n","Epoch: 16, Accuracy: 0.9383\n","Epoch: 17, Accuracy: 0.9389\n","Epoch: 18, Accuracy: 0.9393\n","Epoch: 19, Accuracy: 0.94\n","Epoch: 20, Accuracy: 0.9407\n","Epoch: 21, Accuracy: 0.9418\n","Epoch: 22, Accuracy: 0.9419\n","Epoch: 23, Accuracy: 0.9429\n","Epoch: 24, Accuracy: 0.9435\n","Epoch: 25, Accuracy: 0.9441\n","Epoch: 26, Accuracy: 0.9445\n","Epoch: 27, Accuracy: 0.9445\n","Epoch: 28, Accuracy: 0.9446\n","Epoch: 29, Accuracy: 0.9447\n","Epoch: 30, Accuracy: 0.9442\n","Epoch: 31, Accuracy: 0.9445\n","Epoch: 32, Accuracy: 0.9444\n","Epoch: 33, Accuracy: 0.9447\n","Epoch: 34, Accuracy: 0.9447\n","Epoch: 35, Accuracy: 0.9448\n","Epoch: 36, Accuracy: 0.9448\n","Epoch: 37, Accuracy: 0.945\n","Epoch: 38, Accuracy: 0.9458\n","Epoch: 39, Accuracy: 0.9456\n","Epoch: 40, Accuracy: 0.9461\n","Epoch: 41, Accuracy: 0.9458\n","Epoch: 42, Accuracy: 0.9455\n","Epoch: 43, Accuracy: 0.9459\n","Epoch: 44, Accuracy: 0.9456\n","Epoch: 45, Accuracy: 0.9458\n","Epoch: 46, Accuracy: 0.9456\n","Epoch: 47, Accuracy: 0.9458\n","Epoch: 48, Accuracy: 0.9458\n","Epoch: 49, Accuracy: 0.9462\n","Epoch: 50, Accuracy: 0.9463\n","Epoch: 51, Accuracy: 0.9462\n","Epoch: 52, Accuracy: 0.9461\n","Epoch: 53, Accuracy: 0.9459\n","Epoch: 54, Accuracy: 0.9461\n","Epoch: 55, Accuracy: 0.9461\n","Epoch: 56, Accuracy: 0.9463\n","Epoch: 57, Accuracy: 0.9465\n","Epoch: 58, Accuracy: 0.9466\n","Epoch: 59, Accuracy: 0.9467\n","Epoch: 60, Accuracy: 0.9461\n","Epoch: 61, Accuracy: 0.9463\n","Epoch: 62, Accuracy: 0.9463\n","Epoch: 63, Accuracy: 0.9465\n","Epoch: 64, Accuracy: 0.9465\n","Epoch: 65, Accuracy: 0.9467\n","Epoch: 66, Accuracy: 0.9463\n","Epoch: 67, Accuracy: 0.9461\n","Epoch: 68, Accuracy: 0.9462\n","Epoch: 69, Accuracy: 0.9465\n","Epoch: 70, Accuracy: 0.9468\n","Epoch: 71, Accuracy: 0.9467\n","Epoch: 72, Accuracy: 0.9467\n","Epoch: 73, Accuracy: 0.9469\n","Epoch: 74, Accuracy: 0.947\n","Epoch: 75, Accuracy: 0.9471\n","Epoch: 76, Accuracy: 0.9472\n","Epoch: 77, Accuracy: 0.9473\n","Epoch: 78, Accuracy: 0.9473\n","Epoch: 79, Accuracy: 0.947\n","Epoch: 80, Accuracy: 0.947\n","Epoch: 81, Accuracy: 0.9473\n","Epoch: 82, Accuracy: 0.9479\n","Epoch: 83, Accuracy: 0.9478\n","Epoch: 84, Accuracy: 0.9476\n","Epoch: 85, Accuracy: 0.9476\n","Epoch: 86, Accuracy: 0.9477\n","Epoch: 87, Accuracy: 0.9474\n","Epoch: 88, Accuracy: 0.9474\n","Epoch: 89, Accuracy: 0.9475\n","Epoch: 90, Accuracy: 0.9477\n","Epoch: 91, Accuracy: 0.9477\n","Epoch: 92, Accuracy: 0.9477\n","Epoch: 93, Accuracy: 0.9477\n","Epoch: 94, Accuracy: 0.9477\n","Epoch: 95, Accuracy: 0.9476\n","Epoch: 96, Accuracy: 0.9477\n","Epoch: 97, Accuracy: 0.9475\n","Epoch: 98, Accuracy: 0.9475\n","Epoch: 99, Accuracy: 0.9475\n"]}]},{"cell_type":"markdown","metadata":{"id":"yJM99C7WhbXy"},"source":["Weight Decay + Momentum"]},{"cell_type":"code","metadata":{"id":"-SH4kStlhaiW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKTiU9MH2JmT"},"source":["Inne wersje backprop"]},{"cell_type":"code","metadata":{"id":"BKBj9u_cX6-O"},"source":["    # def backprop(self, x, y):\n","    #     # For a single input (x,y) return a pair of lists.\n","    #     # First contains gradients over biases, second over weights.\n","    #     g = x\n","    #     gs = [g] # list to store all the gs, layer by layer\n","    #     fs = [] # list to store all the fs, layer by layer\n","    #     for b, w in zip(self.biases, self.weights):\n","    #         f = np.dot(w, g)+b\n","    #         fs.append(f)\n","    #         g = sigmoid(f)\n","    #         gs.append(g)\n","    #     # backward pass <- both steps at once\n","    #     dLdg = self.cost_derivative(gs[-1], y)\n","    #     dLdfs = []\n","    #     for w,g in reversed(list(zip(self.weights,gs[1:]))):\n","    #         dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n","    #         dLdfs.append(dLdf)\n","    #         dLdg = np.matmul(w.T, dLdf)\n","        \n","    #     dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] \n","    #     dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] \n","    #     return (dLdBs,dLdWs)\n","\n","\n","    # def backprop(self, x_batch, y_batch):\n","    #     # For a single input (x,y) return a tuple of lists.\n","    #     # First contains gradients over biases, second over weights.\n","        \n","    #     # First initialize the list of gradient arrays\n","    #     delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n","    #     delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n","        \n","    #     # Then go forward remembering all values before and after activations\n","    #     # in two other array lists\n","    #     a = x_batch\n","    #     post_act = []\n","    #     for w, b in zip(self.weights, self.biases):\n","    #       a = sigmoid(np.dot(w, a) + b)\n","    #       post_act.append(a)\n","        \n","    #     # Now go backward from the final cost applying backpropagation\n","    #     ph2 = np.multiply((post_act[1] - y_batch), np.multiply(post_act[1], (1 - post_act[1])))\n","    #     delta_nabla_b[1] += np.sum(ph2,axis=1).reshape(ph2.shape[0],1)#ph2.sum(axis=1)[:, np.newaxis]\n","    #     delta_nabla_w[1] += np.matmul(ph2, post_act[0].T) #+ 0.0001 * x_batch.shape[1] * self.weights[1]\n","        \n","    #     ph1 = np.multiply(np.matmul(self.weights[1].T, ph2), np.multiply(post_act[0], (1 - post_act[0])))\n","    #     delta_nabla_b[0] += np.sum(ph1,axis=1).reshape(ph1.shape[0],1)#ph1.sum(axis=1)[:, np.newaxis]\n","    #     delta_nabla_w[0] += np.matmul(ph1, x_batch.T) #+ 0.0001 * x_batch.shape[1] * self.weights[0]\n","\n","    #     return (delta_nabla_b, delta_nabla_w)"],"execution_count":null,"outputs":[]}]}